import streamlit as st
import os
from rag import PDFRAGSystem
import time

# Configuration de la page
st.set_page_config(
    page_title="Chatbot PDF Local ü§ñ",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .subtitle {
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .model-info {
        background-color: #e8f4fd;
        padding: 12px;
        border-radius: 8px;
        border-left: 4px solid #1f77b4;
        margin: 10px 0;
    }
    .status-connected {
        color: #28a745;
        font-weight: bold;
    }
    .status-disconnected {
        color: #dc3545;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# Chemin sp√©cifique vers votre document PDF
PDF_PATH = "mon rapport.pdf" 

@st.cache_resource
def load_rag_system():
    """Charge le syst√®me RAG avec le chemin sp√©cifique du document"""
    
    if not os.path.exists(PDF_PATH):
        st.error(f"""
        ‚ùå **Document PDF non trouv√© !**
        
        Le fichier sp√©cifi√© n'existe pas:
        `{PDF_PATH}`
        
        Veuillez:
        1. V√©rifier que le fichier existe √† cet emplacement
        2. Ou modifier la variable `PDF_PATH` dans app.py
        3. Assurez-vous que le document est bien un PDF valide
        """)
        return None
    
    try:
        with st.spinner("üîÑ Initialisation du syst√®me RAG avec les mod√®les locaux..."):
            rag_system = PDFRAGSystem(PDF_PATH)
            
            # Tester la connexion Ollama
            if not rag_system.test_ollama_connection():
                st.error("""
                ‚ùå **Ollama n'est pas d√©marr√© !**
                
                Veuillez lancer Ollama avec la commande:
                ```bash
                ollama serve
                ```
                """)
                return None
            
            rag_system.create_embeddings()
            st.success(f"‚úÖ Document charg√©: `{os.path.basename(PDF_PATH)}`")
            return rag_system
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement: {str(e)}")
        return None

def main():
    # Header
    st.markdown('<h1 class="main-header">ü§ñ Chatbot PDF Local</h1>', unsafe_allow_html=True)
    st.markdown('<p class="subtitle">Powered by Ollama ‚Ä¢ mxbai-embed-large ‚Ä¢ llama3.2</p>', unsafe_allow_html=True)
    
    # Afficher le chemin du document
    st.info(f"**Document utilis√©:** `{PDF_PATH}`")
    
    # Chargement du syst√®me
    rag_system = load_rag_system()
    
    # Sidebar
    with st.sidebar:
        st.header("üîß Configuration")
        
        if rag_system:
            info = rag_system.get_model_info()
            
            st.markdown("### üìä Mod√®les Locaux")
            st.markdown(f"""
            <div class="model-info">
            <strong>Embedding:</strong> {info['embedding_model']}<br>
            <strong>G√©n√©ration:</strong> {info['generation_model']}<br>
            <strong>Dimensions:</strong> {info['embedding_dimension']}<br>
            <strong>Segments:</strong> {info['chunks_count']}<br>
            <strong>Statut Ollama:</strong> <span class="status-connected">{info['ollama_status']}</span>
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("---")
        st.header("üéØ Contr√¥les")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üóëÔ∏è Effacer Chat", use_container_width=True):
                if "messages" in st.session_state:
                    st.session_state.messages = []
                st.rerun()
        
        with col2:
            if st.button("üîÑ Recharger", use_container_width=True):
                st.cache_resource.clear()
                st.rerun()
        
        st.markdown("---")
        st.header("‚ùì Exemples de Questions")
        st.markdown("""
        - *"Qu‚Äôest-ce que l‚ÄôIA explicable (XAI) ?"*
        - *"Pourquoi l‚ÄôIA explicable est-elle importante selon le rapport"*
        - *"Qu‚Äôest-ce que LIME, et comment √ßa fonctionne ?"*
        - *"Qu‚Äôest-ce que SHAP et d‚Äôo√π vient ce concept ?"*
        - *"Quel est le ‚Äútrade-off‚Äù (compromis) entre performance du mod√®le et explicabilit√© ?"*
        - *"Comment la XAI peut contribuer √† l‚Äô√©thique et √† la responsabilit√© dans les syst√®mes IA ?"*
        """)
    
    # Interface de chat principale
    if rag_system is None:
        return
    
    # Initialisation de l'historique
    if "messages" not in st.session_state:
        st.session_state.messages = [{
            "role": "assistant", 
            "content": f"üëã Bonjour ! Je suis votre assistant IA local. Je peux r√©pondre √† vos questions sur le document **{os.path.basename(PDF_PATH)}** en utilisant **mxbai-embed-large** pour la recherche et **llama3.2** pour la g√©n√©ration. Posez-moi une question !"
        }]
    
    # Affichage de l'historique
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input utilisateur
    if prompt := st.chat_input("üí≠ Posez votre question sur le document..."):
        # Ajout du message utilisateur
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Affichage du message utilisateur
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # G√©n√©ration de la r√©ponse
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("üîÑ **Recherche dans le document...**")
            
            try:
                # Ajouter un petit d√©lai pour l'UX
                time.sleep(0.5)
                
                # G√©n√©ration de la r√©ponse
                full_response = rag_system.ask_question(prompt)
                
                # Affichage de la r√©ponse
                message_placeholder.markdown(full_response)
                
                # Ajout √† l'historique
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": full_response
                })
                
            except Exception as e:
                error_msg = f"‚ùå **Erreur:** {str(e)}"
                message_placeholder.markdown(error_msg)
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": error_msg
                })

if __name__ == "__main__":
    main()